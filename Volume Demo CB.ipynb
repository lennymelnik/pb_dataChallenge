{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "06c61168ccea0d1c849a29b8bdb5499d01763e17"
   },
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business problem:\n",
    "Pitney Bowes FDR business allows retailers to easily facilitate the return of merchandise (other services are Fullfilment and Delivery). Consumers can drop off the return parcels at any USPS office, and the parcel gets transported to the closest FDR hub to be inducted into the FDR network.\n",
    "\n",
    "Once inducted into our network, the parcel get transported through the FDR network to the client's warehouse.\n",
    "\n",
    "The volume of parcels that is being delivered to the clients' warehouse is driven by external factors like seasonal sales, but also by warehouse schedules, holidays etc.\n",
    "\n",
    "Being able to give our clients reliable forecasts on what parcel volumes they can expect to arrive at their warehouse is very important for them, since the need these forecasts to staff the warehouse accordingly and efficiently manage the available warehouse space.\n",
    "\n",
    "In this challenge, you are given 3 month of historic data for packages that have been delivered to a single client warehouse. <b>The aim is to use this historic data to predict the parcel volumes to arrive at the client's facility over the next 5 days (i.e., 5 numbers for Monday through Friday).</b>\n",
    "\n",
    "#### Data:\n",
    "\n",
    "The data is <b>aggregated by delivery or indiction date</b>. The <b>delivery column</b> shows the total number of parcels that have neem delivered to the client on any given day. The <b>induction columns</b> give you the induction volume per day accross our FDR facilities. The time it takes a parcel to travel from the induction facility to the client facility depends largely on the distance between these facilities.\n",
    "\n",
    "\n",
    "#### Output Format:\n",
    "TBD, but all we need are your <b>5 parcel-volume predictions for June, 3rd - June, 7th 2019</b>. To evaluate your preditcions, we use the Mean Absolute Percentage Error.\n",
    "\n",
    "$MAPE = \\frac{100}{n}\\sum\\limits_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{y_i}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('work/shared'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T14:21:42.956848Z",
     "start_time": "2020-04-17T14:21:42.108587Z"
    },
    "_uuid": "1c434f30fff12aea5c71e7006a582205e0ed3ddf",
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import warnings                                  # `do not disturbe` mode\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np                               # vectors and matrices\n",
    "import pandas as pd                              # tables and data manipulations\n",
    "import matplotlib.pyplot as plt                  # plots\n",
    "\n",
    "from dateutil.relativedelta import relativedelta # working with dates with style\n",
    "from scipy.optimize import minimize              # for function minimization\n",
    "\n",
    "import scipy.stats as scs\n",
    "\n",
    "from itertools import product                    # some useful functions\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T14:21:44.100501Z",
     "start_time": "2020-04-17T14:21:43.990304Z"
    },
    "_uuid": "291830c0c03523b64a5bfdd3ada979f6b6e8e6fe"
   },
   "outputs": [],
   "source": [
    "#Reading the provided csv file into a pandas Dataframe\n",
    "Delivery = pd.read_csv('~/work/shared/Delivery_Volume.csv', index_col=['DELIVERY_DATE'], parse_dates=['DELIVERY_DATE'])\n",
    "Delivery.sort_index(inplace=True)\n",
    "Delivery=Delivery.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T14:21:46.589427Z",
     "start_time": "2020-04-17T14:21:46.510266Z"
    }
   },
   "outputs": [],
   "source": [
    "#Data Sample\n",
    "Delivery.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T02:03:07.352702Z",
     "start_time": "2020-04-17T02:03:06.940690Z"
    },
    "_uuid": "a1d67db8570fdf260088bcd9085d73d8f6d6a735"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "#Delivery.plot.bar(figsize=(10,5))\n",
    "plt.bar(Delivery.index,Delivery.DELIVERED_VOLUME)\n",
    "plt.title('Volume Delivered by Date')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows the delivery volumes at the client facility per day. The client facility is closed on the Saturday and Sundays, which is why you don't see any deliveries on the weekends. There were no deliveries on Memorial Day either. As you can see, there is a strong seasonal pattern the volume of parcels fluctuates quite a bit by weekday. This is why the clients are dependent on volume forecasts so they can manage their warehouse accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b5a77368435753b7aa512dfd96325f21e6beb00"
   },
   "source": [
    "# Example\n",
    "## Moving Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different ways in which you could try to tackle this problem.\n",
    "A very simple approch would be to estimate the next days' volume by using the last days' volume. Given the daily fluctuation of volume, it is obvious that this is not a very promising approach. Another simple way would be to look at moving averages to smoothen out the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T02:03:12.743728Z",
     "start_time": "2020-04-17T02:03:12.739671Z"
    },
    "_uuid": "02e298fc74603ac05bdc34c69ca82456be2059d7",
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def moving_average(series, n):\n",
    "    \"\"\"\n",
    "        Calculate average of last n observations\n",
    "    \"\"\"\n",
    "    return np.average(series[-n:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T03:03:35.414643Z",
     "start_time": "2020-04-17T03:03:35.349106Z"
    },
    "_uuid": "05c627011b7a8b5431c5abacfbc4cca322715bec",
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plotMovingAverage(series, window, plot_intervals=False, scale=1.96, plot_anomalies=False,day_out=5):\n",
    "\n",
    "    \"\"\"\n",
    "        series - dataframe with timeseries\n",
    "        window - rolling window size \n",
    "        plot_intervals - show confidence intervals\n",
    "        plot_anomalies - show anomalies \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    Delivery['SMA_{}'.format(day_out)]=0\n",
    "    for i in range(0,Delivery.shape[0]-day_out,day_out):\n",
    "        for j in range(day_out):\n",
    "            sum_val=0\n",
    "            for k in range(window):\n",
    "                sum_val+= Delivery.iloc[i+k,0]\n",
    "            Delivery.loc[Delivery.index[i+j],'SMA_{}'.format(day_out)] = np.round((sum_val/window),1)\n",
    "    rolling_mean=Delivery['SMA_{}'.format(day_out)]\n",
    "\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.title(\"Moving average\\n window size = {}\".format(window))\n",
    "    plt.plot(rolling_mean, \"g\", label=\"Rolling mean trend\")\n",
    "\n",
    "    # Plot confidence intervals for smoothed values\n",
    "    if plot_intervals:\n",
    "        mae = mean_absolute_error(series[window:].iloc[:,0], rolling_mean[window:])\n",
    "        deviation = np.std(series[window:].iloc[:,0] - rolling_mean[window:])\n",
    "        lower_bond = rolling_mean - (mae + scale * deviation)\n",
    "        upper_bond = rolling_mean + (mae + scale * deviation)\n",
    "        plt.plot(upper_bond, \"r--\", label=\"Upper Bond / Lower Bond\")\n",
    "        plt.plot(lower_bond, \"r--\")\n",
    "        \n",
    "        # Having the intervals, find abnormal values\n",
    "        if plot_anomalies:\n",
    "            anomalies = pd.DataFrame(index=series.index, columns=series.columns)\n",
    "            anomalies[series<lower_bond] = series[series<lower_bond]\n",
    "            anomalies[series>upper_bond] = series[series>upper_bond]\n",
    "            plt.plot(anomalies, \"ro\", markersize=10)\n",
    "        \n",
    "    plt.plot(series[window:].iloc[:,0], label=\"Actual values\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters:\n",
    "- window - no of days to consider when calculating the average\n",
    "- day_out - the number of days for which you are generating the forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what you get when you calculate the average using the volumes delivered over the last 2 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T03:03:39.743761Z",
     "start_time": "2020-04-17T03:03:39.392864Z"
    },
    "_uuid": "4775d01ad54f45cbb3770bd2e3a92d2d503f92f7"
   },
   "outputs": [],
   "source": [
    "plotMovingAverage(Delivery,2,day_out=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "126dd8d923f859005e0cdd00243ae40cdb49638f"
   },
   "source": [
    "As you can see, the forecast is a bit more smooth, but it misses the peaks in delivery volume. Now let's try doing so using the volumes over the previous 7 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T03:03:45.777622Z",
     "start_time": "2020-04-17T03:03:45.482942Z"
    },
    "_uuid": "d7ffebb1a55d81c4e54a53c6e05a84bd081ec35e"
   },
   "outputs": [],
   "source": [
    "plotMovingAverage(Delivery, 7,day_out=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e66c119b2ebfcfb05d9bd16793307aeedcb89660"
   },
   "source": [
    "As expected, the predictions smoothen out more, but the also become less usefull since they eventually converge to the average daily delivery volume. Our clients need more accurate forecasts for the individual days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes and some ideas:\n",
    "- You are not limited to the 13 columns (date, delivery total, induction totals for 11 facilities) provided in the data set, you can generate additional features that might be useful for your predictions. For instance, you could add a feature like weekday to your model. This might be helpful to capture weekday-specific patterns.\n",
    "\n",
    "- Lag variables are very usefull for forecast problems. For instance, you could create additional columns like yesterdays volume (day-1), the day before yesterday (day-2), etc. This might be helpful when trying to use the induction volumes for your model, as they delivery volume follow the induction volumes (with a few days lag depending on where the induction happened).\n",
    "\n",
    "- When building your model, make sure that you get a good understanding of the model's performance by using the historic data for back-testing. Common methods for forecasting problems are rolling window approaches.\n",
    "\n",
    "- The examples above are very basic examples for illustration. You could for instance explore time-series packages that are readily availabe, or try to build a ML model that makes use of additional features you generate.\n",
    "\n",
    "### Reach out if you have questions:\n",
    "<span style=\"color:red\">**Through which channel should they reach out?**</span>\n",
    "\n",
    "### Further reading:\n",
    "- [Time Series](https://www.kaggle.com/kashnitsky/topic-9-part-1-time-series-analysis-in-python/notebook)\n",
    "- [Jupyter Notebooks](https://www.dataquest.io/blog/jupyter-notebook-tutorial/)\n",
    "\n",
    "## Good luck!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
